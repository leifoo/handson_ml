{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 11. Training Deep Neural Nets\n",
    "\n",
    "Difficulties of large deep neural network:\n",
    "* _Vanishing gradients problem_ (or _exploding gradients problem_) makes lower layers very hard to train.\n",
    "* Training is extremely slow.\n",
    "* Risk overfitting.\n",
    "\n",
    "### Vanishing/Exploding Gradients Problems\n",
    "\n",
    "_Vanishing gradients_ problem: gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layer connection weights virtually unchanged, and training never converges to a good solution.\n",
    "\n",
    "_Exploding gradient_ problem: the gradients can grow bigger and bigger, so many layers get insanely large weight updates and the algorithm diverges. (mostly encountered in recurrent neural networks)\n",
    "\n",
    "More generally, deep neural networks suffer from unstable gradients; different layers may learn at widely different speeds.\n",
    "\n",
    "\"Understanding the Difficulty of Training Deep Feedforward Neural Networks\" by Xavier Glorot and Yoshua Bengio found a few suspects, including the combination of the popular logistic sigmoid activation function and the weight initialization technique that was most popular at the time, namely random initialization using a normal distribution with a mean of 0 and a standard deviation of 1. In short, they showed that with this activation function and this initialization scheme, the variance of the outputs of each layer is much greater than the variance of its inputs. \n",
    "\n",
    "#### Xavier and He Initialization\n",
    "\n",
    "For the signal to flow properly, the variance of the outputs of each layer is need to be equal to the variance of its inputs, and we also need the gradients to have equal variance before and after flowing through a layer in the reverse direction. The connection weights must be initialized randomly as \n",
    "\n",
    "_Xavier (Glorot) initialization (when using the logistic activation function)_\n",
    "\n",
    ">Normal distribution with mean $0$ and standard deviation $\\ \\sigma = \\sqrt{\\frac{2}{n_{in}+n_{out}}}$\n",
    "<br>\n",
    ">or a uniform distribution between $-r$ and $+r$, with $\\ r=\\sqrt{\\frac{6}{n_{in}+n_{out}}}$\n",
    "\n",
    "   where $n_{in}$ and $n_{out}$ are the number of input and output connections for the layer whose weights are being initialized (also called _fan-in_ and _fan-out_).\n",
    "\n",
    "When $n_{in} \\approx n_{out}$,\n",
    "\n",
    "$$\\sigma = \\frac{1}{\\sqrt{n}} \\ \\ \\ or \\ \\ \\ r = \\frac{\\sqrt{3}}{\\sqrt{n}}$$\n",
    "\n",
    "The initialization strategy for the ReLU activation function (and its variants, including the ELU activation) is sometimes called _He initialization_.\n",
    "\n",
    "<div style=\"width:400 px; font-size:100%; text-align:center;\"> <center><img src=\"img/tab11-1.png\" width=400px alt=\"tab11-1\" style=\"padding-bottom:1.0em;padding-top:2.0em;\"></center>_Table 11-1. Initialization parameters for each type of activation function_</div>\n",
    "\n",
    "Use He initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "he_init = tf.variance_scaling_initializer()\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                          kernel_initializer=he_init, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>_NOTE_</font>\n",
    ">He initialization considers only the fan-in, not the average between fan-in and fan-out like in Xavier initialization. This is also the default for the `variance_scaling_initializer()` function, but you can change this by setting the argument `mode=\"FAN_AVG\"`.\n",
    "\n",
    "#### Nonsaturating Activation Functions\n",
    "\n",
    "ReLU activation function is much better than sigmoid activation function, because it does not saturate for positive values (and also because it is quite fast to compute).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 11. Training Deep Neural Nets\n",
    "\n",
    "Difficulties of large deep neural network:\n",
    "* _Vanishing gradients problem_ (or _exploding gradients problem_) makes lower layers very hard to train.\n",
    "* Training is extremely slow.\n",
    "* Risk overfitting.\n",
    "\n",
    "### Vanishing/Exploding Gradients Problems\n",
    "\n",
    "_Vanishing gradients_ problem: gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layer connection weights virtually unchanged, and training never converges to a good solution.\n",
    "\n",
    "_Exploding gradient_ problem: the gradients can grow bigger and bigger, so many layers get insanely large weight updates and the algorithm diverges. (mostly encountered in recurrent neural networks)\n",
    "\n",
    "More generally, deep neural networks suffer from unstable gradients; different layers may learn at widely different speeds.\n",
    "\n",
    "\"Understanding the Difficulty of Training Deep Feedforward Neural Networks\" by Xavier Glorot and Yoshua Bengio found a few suspects, including the combination of the popular logistic sigmoid activation function and the weight initialization technique that was most popular at the time, namely random initialization using a normal distribution with a mean of 0 and a standard deviation of 1. In short, they showed that with this activation function and this initialization scheme, the variance of the outputs of each layer is much greater than the variance of its inputs. \n",
    "\n",
    "#### Xavier and He Initialization\n",
    "\n",
    "For the signal to flow properly, the variance of the outputs of each layer is need to be equal to the variance of its inputs, and we also need the gradients to have equal variance before and after flowing through a layer in the reverse direction. The connection weights must be initialized randomly as \n",
    "\n",
    "_Xavier (Glorot) initialization (when using the logistic activation function)_\n",
    "\n",
    ">Normal distribution with mean $0$ and standard deviation $\\ \\sigma^2 = \\frac{1}{fan_{avg}}$\n",
    "<br>\n",
    ">or a uniform distribution between $-r$ and $+r$, with $\\ r=\\sqrt{\\frac{3}{fan_{avg}}}$\n",
    "\n",
    "where $fan_{avg} = (fan_{in} + fan_{out})/2$ is average of the number of input and output connections for the layer whose weights are being initialized (also called *fan-in* and *fan-out*).\n",
    "\n",
    "*LeCun initialization*: replace $fan_{avg}$ with $fan_{in}$\n",
    "\n",
    "\n",
    "The initialization strategy for the ReLU activation function (and its variants, including the ELU activation described shortly) is sometimes called *He* initialization. The SELU activation function should be used with LeCun initialization (preferably with a normal distribution).\n",
    "\n",
    "*Initialization parameters ofr each type of activation function*\n",
    "\n",
    "| Initialization   |   Activation functions   |   $\\sigma^2$ (Normal)   |\n",
    "|------|------|------|\n",
    "| Glorot | None, Tanh, Logistic, Softmax  | $1/fan_{avg}$   | \n",
    "| He     | ReLU & variants | $2/fan_{in}$ |\n",
    "| LeCun  | SELU            | $1/fan_{in}$ |\n",
    "\n",
    "$$\\sigma = \\frac{1}{\\sqrt{n}} \\ \\ \\ or \\ \\ \\ r = \\frac{\\sqrt{3}}{\\sqrt{n}}$$\n",
    "\n",
    "The initialization strategy for the ReLU activation function (and its variants, including the ELU activation) is sometimes called _He initialization_.\n",
    "\n",
    "<div style=\"width:400 px; font-size:100%; text-align:center;\"> <center><img src=\"img/tab11-1.png\" width=400px alt=\"tab11-1\" style=\"padding-bottom:1.0em;padding-top:2.0em;\"></center>Table 11-1. Initialization parameters for each type of activation function</div>\n",
    "\n",
    "Use He initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7fce28626090>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg',\n",
    "distribution='uniform')\n",
    "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>*NOTE*</font>\n",
    ">He initialization considers only the fan-in, not the average between fan-in and fan-out like in Xavier initialization. This is also the default for the `variance_scaling_initializer()` function, but you can change this by setting the argument `mode=\"FAN_AVG\"`.\n",
    "\n",
    "#### Nonsaturating Activation Functions\n",
    "\n",
    "ReLU activation function is much better than sigmoid activation function, because it does not saturate for positive values (and also because it is quite fast to compute).\n",
    "\n",
    "Problem *dying ReLUs*: during training, some neurons effectively die, meaning they stop outputting anything other than $0$. During training, if a neuron's weights get updated such that the weighted sum of the neuron's input is negative, it will start outputting $0$.\n",
    "\n",
    "To solve this problem, use *leaky ReLU*\n",
    "\n",
    "$$ LeakyReLU_\\alpha(z)=max(\\alpha z, z) $$\n",
    "\n",
    "where hyperparameter $\\alpha$ is typically set to $0.01$. \n",
    "- Huge leak ($\\alpha = 0.2$) seemed to result inm better performance than small leak.\n",
    "- *Randomized leak ReLU* (RReLU), where $\\alpha$ is picked randomly in a given range during training, and it fixed to an average value during testing. It also performed well and seemed to act as a regularizer (reducing the risk of overfitting the training set). \n",
    "- *Parametric leak ReLU)* (PReLU), where $\\alpha$ is authorized to be learned during training (instead of a hyperparameter). Strongly outperform ReLU on large image datasets, but on smaller datasets it runs the risk of overfitting the training set.\n",
    "\n",
    "<div style=\"width:400 px; font-size:100%; text-align:center;\"> <center><img src=\"img/fig11-2.png\" width=400px alt=\"fig11-2\" style=\"padding-bottom:1.0em;padding-top:2.0em;\"></center>Figure 11-2. Leaky ReLU</div>\n",
    "\n",
    "*Exponential linear unit* (ELU). \n",
    "\n",
    "$$ELU_\\alpha(z) = \n",
    "\\left\\{\\begin{matrix}\n",
    "\\alpha(exp(z)-1) \\ \\ if \\ z < 0\n",
    "\\\\ \n",
    "z \\qquad \\qquad \\ \\ \\ \\ if \\ z \\ge 0\n",
    "\\end{matrix}\\right.$$\n",
    "\n",
    "Reduced training time and better performed on test set.\n",
    "\n",
    "Major differences with ReLU:\n",
    "- Negative value when $z<0$, which allows the unit to have an average output closer to $0$. This helps alleviate the vanishing gradients problem. Usually, $\\alpha=1$\n",
    "- Nonzero gradient for $z<0$, which avoids the dying units issue.\n",
    "- The function is smooth everywhere, including around $z=0$, which helps speed up Gradient Descent, since it does not bounce as much left and right of $z=0$.\n",
    "\n",
    "Main drawback: slow to compute than ReLU and its variants. During training this is compensated by the faster convergence rate. However, at test time an ELU network is slower than a ReLU network.\n",
    "\n",
    "<font color=blue>*TIP*</font>\n",
    ">ELU > leaky ReLU > ReLU > tanh > logistic. Use cross-validation to evaluate other activation functions, in particular RReLU if your network is overfitting, or PReLU if you have a hu8ge training set.\n",
    "\n",
    "To use the leaky ReLu activation function, you must create a `LeakyReLU` instance like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_relu = keras.layers.LeakyReLU(alpha=0.2)\n",
    "layer =keras.layers.Dense(10, activation=leaky_relu,\n",
    "                          kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELU activation function, a scaled version of the ELU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer =keras.layers.Dense(10, activation=\"selu\",\n",
    "                         kernel_initializer=\"lecun_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Normalization\n",
    "\n",
    "Although using He initialization along with ELU (or any variant of ReLU) can significantly reduce the vanishing/exploring gradients problems at the beginning of training, it doesn't guarantee that they won't come back during training.\n",
    "\n",
    "*Batch Normalization* (BN), addresses the vanishing/exploring gradient problems, and more generally the problem that the distribution of each layer';s inputs changes during training, as the parameters of the previous layers change (*Internal Covariate Shift* problem).\n",
    "\n",
    "In order to zero-center and normalize the inputs, the algorithm needs to evaluating the mean and standard deviation of the inputs over the current mini-batch. \n",
    "\n",
    "Four parameters are learned for each batch-normalized layer:\n",
    "- $\\gamma$, scale\n",
    "- $\\beta$, offset\n",
    "- $\\mu$, mean\n",
    "- $\\sigma$, standard deviation\n",
    "\n",
    "The vanishing gradients problem is strongly reduced. The networks are also much less sensitive to the weight initialization. Be able to use much larger learning rates, significantly speeding up the learning process. BN also acts like a regularizer. \n",
    "\n",
    "Batch Normalization does, however, add some complexity to the model. Moreover, there is runtime penalty: the neural network makes slower prediction due to the extra computations required at each layer. So if you need predictions to be lightening-fast, you may want to check how ell plain ELU + He initialization perform before playing with Batch Normalization. \n",
    "\n",
    "##### Implementing Batch Normalization with TensorFlow\n",
    "\n",
    "TensorFlow provides `tf.nn.batch_normalization()` function, but you must compute the mean and standard deviation yourself. Instead, you should use the `tf.layers.batch_normalization()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BN algorithm uses *exponential decay* to compute the running averages, which is why it requires the *momentum* parameter: given a new value $v$, the running average $\\hat v$ is updated as\n",
    "\n",
    "$$ \\hat v \\leftarrow \\hat v \\times momentum + v \\times (1-momentum)$$\n",
    "\n",
    "#### Gradient Clipping\n",
    "\n",
    "A popular technique to lessen the exploding gradients problems is simply clip the gradients during backpropagation so they never exceed some threshold (mostly useful for recurrent neural network).\n",
    "\n",
    "### Reusing Pretrained Layers\n",
    "\n",
    "*Transfer Learning*: find an existing neural network that accomplishes a similar task, then just reuse its lower layers. \n",
    "\n",
    "Benefits:\n",
    "1. speed up training considerably.\n",
    "2. require much less training data.\n",
    "\n",
    "<font color=blue>*NOTE*</font>\n",
    ">1. If the input size of the new task is different from the one in original task, you need to add a preprocessing step to resize them to the size expected by the original model. \n",
    ">2. Generally, transfer learning will only work well if the inputs have similar low-level features.\n",
    "\n",
    "#### Reusing a TensorFlow Model\n",
    "\n",
    "#### Reusing Models from Other Frameworks\n",
    "\n",
    "#### Freezing the Lower Layers\n",
    "\n",
    "#### Caching the Frozen Layer\n",
    "\n",
    "#### Tweaking, Dropping, or Replacing the Upper Layers\n",
    "\n",
    "#### Model Zoos\n",
    "\n",
    "https://github.com/tensorflow/models\n",
    "\n",
    "https://github.com/ethereon/caffe-tensorflow\n",
    "\n",
    "#### Unsupervised Pretraining\n",
    "\n",
    "If you have plenty of unlabeled training data, train the layers one by one, starting with lowest layer and then going up, using an unsupervised feature detector algorithm such as *Restricted Boltzmann Machines* (RBMs) or autoencoders. Each layer is trained on the output of the previous trained layers (all layers except the one being trained are frozen). Once all layers have been trained this way, you can fine-tune the network using supervised learning (i.e., with backpropagation).\n",
    "\n",
    "#### Pretraining on an Auxiliary Task\n",
    "\n",
    "One last option is to train a first neural network on an auxiliary task for which you can easily obtain or\n",
    "generate labeled training data, then reuse the lower layers of that network for your actual task. The first\n",
    "neural network's lower layers will learn feature detectors that will likely be reusable by the second\n",
    "neural network.\n",
    "\n",
    "*max margin learning*: train a first network to output a score for each training instance, and use a cost\n",
    "function that ensures that a good instance's score is greater than a bad instance's score by at least some\n",
    "margin.\n",
    "\n",
    "### Faster Optimizers\n",
    "\n",
    "Ways to speed up large deep neural network training:\n",
    "- Apply a good initialization strategy for the connection weights\n",
    "- Use a good activation function\n",
    "- Use Batch Normalization\n",
    "- Reuse parts of the pretrained network (possibly built on a auxiliary task or using unsupervised learning)\n",
    "- Faster optimizer than the regular Gradient Descent optimizer (Momentum optimization, Nesterov Accelerated Gradient, AdaGrad, RMSProp, and Adam and Nadam optimization).\n",
    "\n",
    "#### Momentum Optimization\n",
    "\n",
    "\\begin{equation} \\label{eq1}\n",
    "\\begin{split}\n",
    "& 1. \\ \\ \\mathbf{m} \\leftarrow \\beta \\mathbf{m} - \\eta \\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta})  \\\\\n",
    "& 2. \\ \\ \\mathbf{\\theta} \\leftarrow \\mathbf{\\theta} + \\mathbf{m}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "- $\\mathbf{m}$ - *momentum vector* \n",
    "- $\\beta$ - *momentum*. 0 - hight friction; 1 - no friction; typical 0.9\n",
    "\n",
    "In deep neural networks that don't use Batch Normalization, the upper layers will often end up having inputs with very different scales, so using Momentum optimization helps a lot. It can also help roll past local optima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nesterov Accelerated Gradient\n",
    "\n",
    "\\begin{equation} \\label{eq2}\n",
    "\\begin{split}\n",
    "& 1. \\ \\ \\mathbf{m} \\leftarrow \\beta \\mathbf{m} - \\eta \\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta}+\\beta \\mathbf{m})  \\\\\n",
    "& 2. \\ \\ \\mathbf{\\theta} \\leftarrow \\mathbf{\\theta} + \\mathbf{m}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "The idea is to measure the gradient of the cost function not at the local position but slightly ahead in the direction of the momentum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaGrad\n",
    "\n",
    "\\begin{equation} \\label{eq3}\n",
    "\\begin{split}\n",
    "& 1. \\ \\ \\mathbf{s} \\leftarrow \\mathbf{s} + \\nabla_{\\mathbf{\\theta}}(\\mathbf{\\theta}) \\otimes \\nabla_{\\mathbf{\\theta}}(\\mathbf{\\theta}) \\\\\n",
    "& 2. \\ \\ \\mathbf{\\theta} \\leftarrow \\mathbf{\\theta} - \\eta \\nabla_{\\mathbf{\\theta}} \\oslash \\sqrt{\\mathbf{s}+\\epsilon}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "The algorithm can correct its direction to point a bit more toward the global optimum by scaling down the gradient vector along the steepest dimensions.\n",
    "\n",
    "In short, this algorithm decays the learning rate, but it does so faster for steep dimensions than for dimensions with gentler slopes. This is called an adaptive learning rate. It helps point the resulting updates more directly toward the global optimum. One additional benefit is that it requires much less tuning of the learning rate hyperparameter $\\eta$.\n",
    "\n",
    "AdaGrad often performs well for simple quadratic problems, but unfortunately it often stops too early when training neural networks. The learning rate gets scaled down so much that the algorithm ends up stopping entirely before reaching the global optimum. So even though Keras has an `Adagrad` optimizer, you should not use it to train deep neural networks\n",
    "\n",
    "#### RMSProp\n",
    "\n",
    "\\begin{equation} \\label{eq4}\n",
    "\\begin{split}\n",
    "& 1. \\ \\ \\mathbf{s} \\leftarrow \\beta \\mathbf{s} + (1-\\beta) \\nabla_{\\mathbf{\\theta}}(\\mathbf{\\theta}) \\otimes \\nabla_{\\mathbf{\\theta}}(\\mathbf{\\theta}) \\\\\n",
    "& 2. \\ \\ \\mathbf{\\theta} \\leftarrow \\mathbf{\\theta} - \\eta \\nabla_{\\mathbf{\\theta}} \\oslash \\sqrt{\\mathbf{s}+\\epsilon}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Although AdaGrad slows down a bit too fast and ends up never converging to the global optimum, the RMSProp algorithm fixes this by accumulating only the gradients from the most recent iterations (as opposed to all the gradients since the beginning of training). It does so by using exponential decay in the first step.\n",
    "\n",
    "The decay rate $\\beta$ is typically set to 0.9. \n",
    "\n",
    "Except on very simple problems, this optimizer almost always performs much better than AdaGrad. In fact, it was the preferred optimization algorithm of many researchers until Adam optimization came around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam and Nadam Optimization\n",
    "\n",
    "\\begin{equation} \\label{eq5}\n",
    "\\begin{split}\n",
    "& 1. \\ \\ \\mathbf{m} \\leftarrow \\beta_1 \\mathbf{m} - (1-\\beta_1) \\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta}+\\beta \\mathbf{m})  \\\\\n",
    "& 2. \\ \\ \\mathbf{s} \\leftarrow \\beta_2 \\mathbf{s} + (1-\\beta_2) \\nabla_{\\mathbf{\\theta}}(\\mathbf{\\theta}) \\otimes \\nabla_{\\mathbf{\\theta}}(\\mathbf{\\theta}) \\\\\n",
    "& 3. \\ \\ \\hat{\\mathbf{m}} \\leftarrow \\frac{\\mathbf{m}}{1-\\beta_1^t} \\\\\n",
    "& 4. \\ \\ \\hat{\\mathbf{s}} \\leftarrow \\frac{\\mathbf{s}}{1-\\beta_2^t} \\\\\n",
    "& 5. \\ \\ \\mathbf{\\theta} \\leftarrow \\mathbf{\\theta} + \\eta \\hat{\\mathbf{m}} \\oslash \\sqrt{\\hat{\\mathbf{s}}+\\epsilon}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "*Adam*: adaptive moment estimation.\n",
    "\n",
    "It combines the ideas of Momentum optimization and RMSProp: just like Momentum optimization it keeps track of\n",
    "an exponentially decaying average of past gradients, and just like RMSProp it keeps track of an exponentially decaying average of past squared gradients.\n",
    "\n",
    "Since Adam is an adaptive learning rate algorithm (like AdaGrad and RMSProp), it requires less tuning of the learning rate hyperparameter $\\eta$. You can often use the default value $\\eta = 0.001$, making Adam even easier to use than Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rate Scheduling\n",
    "\n",
    "Finding a good learning rate can be tricky. If you set it way too high, training may actually diverge. If you set it too low, training will eventually converge to the optimum, but it will take a very long time. If you set it\n",
    "slightly too high, it will make progress very quickly at first, but it will end up dancing around the optimum, never really settling down. If you have a limited computing budget, you may have to interrupt training before it has converged properly, yielding a suboptimal solution.\n",
    "\n",
    "<div style=\"width:400 px; font-size:100%; text-align:center;\"> <center><img src=\"img/fig11-8.png\" width=400px alt=\"fig11-8\" style=\"padding-bottom:1.0em;padding-top:2.0em;\"></center>Figure 11-8. Learning curves for various learning rates $\\eta$</div>\n",
    "\n",
    "Learning Schedules:\n",
    "- Power scheduling\n",
    "- Exponential scheduling\n",
    "- Piecewise constant scheduling\n",
    "- Performance scheduling\n",
    "\n",
    "### Avoiding Overfitting Through Regularization\n",
    "\n",
    "Early stopping in Chapter 10 and Batch Normalization.\n",
    "\n",
    "#### $l_1$ and $l_2$ Regularization\n",
    "\n",
    "Use $l_1$ and $l_2$ regularization to constrain a neural network's connection weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                           activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01)) # l1_l2\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\",\n",
    "                     kernel_initializer=\"glorot_uniform\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout\n",
    "\n",
    "*Dropout* is one of the most popular regularization techniques for deep neural networks.\n",
    "\n",
    "It is a fairly simple algorithm: at every training step, every neuron (including the input neurons, but always excluding the output neurons) has a probability $p$ of being temporarily \"dropped out,\" meaning it will be entirely ignored during this training step, but it may be active during the next step (see Figure 11-9). The hyperparameter $p$ is called the *dropout rate*, and it is typically set to $50%$. After training, neurons don't\n",
    "get dropped anymore.\n",
    "\n",
    "<div style=\"width:400 px; font-size:100%; text-align:center;\"> <center><img src=\"img/fig11-9.png\" width=400px alt=\"fig11-8\" style=\"padding-bottom:1.0em;padding-top:2.0em;\"></center>Figure 11-9. Dropout regularization</div>\n",
    "\n",
    "Neurons trained with dropout cannot\n",
    "co-adapt with their neighboring neurons; they have to be as useful as possible on their own. They also cannot rely excessively on just a few input neurons; they must pay attention to each of their input neurons. They end up being less sensitive to slight changes in the inputs. In the end you get a more robust network that generalizes better.\n",
    "\n",
    "Another way to understand the power of dropout is to realize that a unique neural network is generated at each training step. Since each neuron can be either present or absent, there is a total of $2^N$ possible networks (where $N$ is the total number of droppable neurons). This is such a huge number that it is virtually impossible for the same neural network to be sampled twice. Once you have run a 10,000 training steps, you have essentially trained 10,000 different neural networks (each with just one training instance). These neural networks are obviously not independent since they share many of their weights, but they are nevertheless all different. The resulting neural network can be seen as an averaging **ensemble** of all these smaller neural networks.\n",
    "\n",
    "There is one small but important technical detail. Suppose $p = 50%$, in which case during testing a neuron will be connected to twice as many input neurons as it was (on average) during training. To compensate for this fact, we need to multiply each neuron's input connection weights by 0.5 after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you observe that the model is overfitting, you can increase the dropout rate. Conversely, you should try decreasing the dropout rate if the model underfits the training set. It can also help to increase the dropout rate for large layers, and reduce it for small ones. Moreover, many state-of-the-art architectures only use dropout after the last hidden layer, so you may want to try this if full dropout is too strong.\n",
    "\n",
    "Dropout does tend to significantly slow down convergence, but it usually results in a much better model when tuned properly. So, it is generally well worth the extra time and effort.\n",
    "\n",
    "#### Monte-Carlo (MC) Dropout\n",
    "\n",
    "In short, MC Dropout is a fantastic technique that boosts dropout models and provides better uncertainty estimates. And of course, since it is just regular dropout during training, it also acts like a regularizer.\n",
    "\n",
    "#### Max-Norm Regularization\n",
    "\n",
    "For each neuron, it constrains the weights $w$ of the incoming connections such that $∥ *w* ∥_2 \\le r$, where $r$ is the max-norm hyperparameter.\n",
    "\n",
    "### Summary and Practical Guidelines\n",
    "\n",
    "Don't forget to standardize the input features! Try to reuse parts of a pretrained neural network if you can find one that solves a similar problem, or use unsupervised pretraining if you have a lot of unlabeled data, or pretraining on an auxiliary task if you have a lot of labeled data for a similar task.\n",
    "\n",
    "Default DNN configuration:\n",
    "\n",
    "| Hyperparameter | Default value |\n",
    "|------|------|\n",
    "|   Kernel initializer:  | LeCun initializer|\n",
    "|   Activation function: | SELU |\n",
    "|   Normalization:       | None (self-normalization) |\n",
    "|   Regularization:      | Early stopping |\n",
    "|   Optimizer:           | Nadam |\n",
    "|   Learning rate schedule: | Performance scheduling |\n",
    "\n",
    "- If your model self-normalizes:\n",
    "    - If it overfits the training set, then you should add alpha dropout (and always use early stopping as well). Do not use other regularization methods, or else they would break self-normalization.\n",
    "- If your model cannot self-normalize (e.g., it is a recurrent net or it contains skip connections):\n",
    "     - You can try using ELU (or another activation function) instead of SELU, it may perform better. Make sure to change the initialization method accordingly (e.g., He init for ELU or ReLU).\n",
    "    - If it is a deep network, you should use Batch Normalization after every hidden layer. If it overfits the training set, you can also try using max-norm or l 2 regularization.\n",
    "- If you need a sparse model, you can use l 1 regularization (and optionally zero out the tiny weights after training). If you need an even sparser model, you can try using FTRL instead of Nadam optimization, along with l 1 regularization. In any case, this will break self-normalization, so you will need to switch to BN if your model is deep.\n",
    "- If you need a low-latency model (one that performs lightning-fast predictions), you may need to use less layers, avoid Batch Normalization, and possibly replace the SELU activation function with the leaky ReLU. Having a sparse model will also help. You may also want to reduce the float precision from 32-bits to 16-bit (or even 8-bits).\n",
    "- If you are building a risk-sensitive application, or inference latency is not very important in your application, you can use MC Dropout to boost performance and get more reliable probability estimates, along with uncertainty estimates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

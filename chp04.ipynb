{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4. Training Models\n",
    "\n",
    "Two different ways to train a Linear Regression model:\n",
    " - Use a direct \"closed-form\" equation that directly computes the model parameters that best fit the model to the training set.\n",
    " - Use an iterative optimization approach, Gradient Descent (GD), Batch GD, Mini-batch GD, and Stochastic GD.\n",
    "\n",
    "Polynomial Regression, more prone to overfitting. Detect overfitting using learning curves, reduce using regularization.\n",
    "\n",
    "Logistic Regression\n",
    "\n",
    "Softmax Regression\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "A linear model makes a prediction by simply computing a weighted sum of the input features plus a constant called the bias term (intercept term).\n",
    "\n",
    "$$\\mathbf{y} = h_\\theta(\\mathbf{x}) = \\theta^T \\cdot \\mathbf{x}$$\n",
    "\n",
    "Cost function\n",
    "\n",
    "$$MSE(\\mathbf{X}, h_\\theta) = \\frac{1}{m} \\sum^m_{i=1}(\\theta^T \\cdot \\mathbf{x}^{(i)} - y_{(i)})^2$$\n",
    "\n",
    "#### The Normal Equation\n",
    "\n",
    "Closed-form solution\n",
    "$$\\hat{\\theta} = (\\mathbf{X}^T \\cdot \\mathbf{X})^{-1} \\cdot \\mathbf{X}^T \\cdot \\mathbf{y}$$\n",
    "\n",
    "#### Computational Complexity\n",
    "\n",
    "<font color=red>_WARNING_</font>\n",
    ">The Normal Equation gets very slow when the number of features grows large (e.g., 100,000).\n",
    "\n",
    "On the positive side, this equation is linear with regards to the number of instances and features in the training set (it is $O(m)$), so it handles large training sets efficiently, provided they can fit in memory.\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Concretely, you start by filling $\\theta$ with random values (_random initialization_), and then you improve it gradually, taking one baby step at a time, each step attempting to decrease the cost function, until the algorithm _converges_ to a minimum.\n",
    "\n",
    "_Learning rate_ hyperparameter: small, slow convergence; large, algorithm diverge.\n",
    "\n",
    "Two main challenges with Gradient Descent: \n",
    " - Random initialization, it could converge to a local minimum; \n",
    " - It may take a very long time to cross the plateau, and if you stop too early you will never reach the global minimum.\n",
    "\n",
    "<font color=red>_WARNING_</font>\n",
    ">When using Gradient Descent, you should ensure that all features have a similar scale, or else it will take much longer to converge.\n",
    "\n",
    "#### Batch Gradient Descent\n",
    "\n",
    "It involves calculations over the full training set X, at each Gradient Descent step.\n",
    "\n",
    "How to set the number of iterations: set a very large number of iterations but to interrupt the algorithm when the norm of the gradient vector becomes smaller than $\\epsilon$ (tolerance).\n",
    "\n",
    "#### Stochastic Gradient Descent\n",
    "\n",
    "Stochastic Gradient Descent just picks a random instance in the training set at every step and computes the gradients based only on that single instance. (fast, train on huge training sets)\n",
    "\n",
    "Much less regular than Batch Gradient Descent: instead of gently decreasing until it reaches the minimum, the cost function will bounce up and down, decreasing only on average. Over time it will end up very close to the minimum, but once it gets there it will continue to bounce around, never settling down. So once the algorithm stops, the final parameter values are good, but not optimal.\n",
    "\n",
    "Randomness is good to escape from local optima, but bad because it means that the algorithm can never settle at the minimum. One solution: gradually reduce the learning rate. _Simulated\n",
    "annealing_: the steps start out large (which helps make quick progress and escape local minima), then get smaller and smaller, allowing the algorithm to settle at the global minimum. The function that determines the learning rate at each iteration is called the _learning\n",
    "schedule_.\n",
    "\n",
    "_Epoch_: each round of $m$ iterations.\n",
    "\n",
    "#### Mini-batch Gradient Descent\n",
    "\n",
    "Advantage: performance boost from hardware optimization of matrix operations, especially with GPUs.\n",
    "\n",
    "Less erratic than with SGD, harder for it to escape from local minima.\n",
    "\n",
    "<div style=\"width:600 px; font-size:100%; text-align:center;\"> <center><img src=\"img/tab4-1.png\" width=600px alt=\"tab4-1\" style=\"padding-bottom:1.0em;padding-top:2.0em;\"></center>_Table 4-1. Comparison of algorithms for Linear Regression_</div>\n",
    "\n",
    "### Polynomial Regression\n",
    "\n",
    "Polynomial Regression is capable of finding relationships between features.\n",
    "\n",
    "### Learning Curves\n",
    "\n",
    "One way to estimate a model's generalization performance is using cross-validation.\n",
    "\n",
    "Another way is to look at the _learning curves_: plots of the model's performance on the training set and the validation set as a function of the training set size.\n",
    "\n",
    "<div style=\"width:400 px; font-size:100%; text-align:center;\"> <center><img src=\"img/fig4-15.png\" width=400px alt=\"fig4-15\" style=\"padding-bottom:1.0em;padding-top:2.0em;\"></center>_Figure 4-15. Underfitting model. Both curves have reached a plateau; they are close and fairly high_</div>\n",
    "\n",
    "<div style=\"width:400 px; font-size:100%; text-align:center;\"> <center><img src=\"img/fig4-16.png\" width=400px alt=\"fig4-16\" style=\"padding-bottom:1.0em;padding-top:2.0em;\"></center>_Figure 4-16. Overfitting model._</div>\n",
    "\n",
    "The error on the training data is much lower than with the underfitting model.\n",
    "\n",
    "There is a gap between the curves. This means that the model performs significantly better on the\n",
    "training data than on the validation data, which is the hallmark of an overfitting model. However, if you used a much larger training set, the two curves would continue to get closer.\n",
    "\n",
    "Bias/variance tradeoff\n",
    "\n",
    "A model's generalization error can be expressed as the sum of three very different errors:\n",
    " - Bias. Underfit due to wrong assumption.\n",
    " - Variance. Overfit due to the model's excessive sensitivity to small variation in the training data.\n",
    " - Irreducible error. Due to the noiseness of the data itself, clean up the data.\n",
    " \n",
    "### Regularized Linear Models\n",
    "\n",
    "#### Ridge Regression\n",
    "\n",
    "_Tikhonov regularization_: a _regularization term_ equal to $\\alpha \\sum^n_{i=1} \\theta_i^2$ is added to the cost function.\n",
    "\n",
    "Note that the regularization term should only be added to the cost function during training. Once the model is trained, you want to evaluate the model's performance using the unregularized performance measure.\n",
    "\n",
    "<font color=blue>_NOTE_</font>\n",
    ">It is quite common for the cost function used during training to be different from the performance measure used for testing. Apart from regularization, another reason why they might be different is that a good training cost function should have optimization-friendly derivatives, while the performance measure used for testing should be as close as possible to the final objective.\n",
    "\n",
    "Ridge Regression cost function:\n",
    "$$J(\\theta) = MSE(\\theta) + \\frac{\\alpha}{2} \\sum^n_{i=1} \\theta^2_i$$\n",
    "\n",
    "The penalty hyperparameter $\\alpha$ controls how much you want to regularize the model. Increasing $\\alpha$ leads to flatter (i.e., less extreme, more reasonable) predictions; this reduces the model’s variance but increases its bias.\n",
    "\n",
    "<font color=red>_WARNING_</font>\n",
    ">It is important to scale the data before performing Ridge Regression, as it is sensitive to the scale of the input features. This is true of most regularized models.\n",
    "\n",
    "#### Lasso Regression\n",
    "\n",
    "_Least Absolute Shrinkage and Selection Operator Regression_, $l_1$ norm.\n",
    "\n",
    "$$J(\\theta) = MSE(\\theta) + \\alpha \\sum^n_{i=1} | \\theta_i |$$\n",
    "\n",
    "An important characteristic of Lasso Regression is that it tends to completely eliminate the weights of the least important features (i.e., set them to zero). In other words, Lasso Regression automatically performs feature selection and outputs a _sparse model_ (i.e., with few nonzero feature weights).\n",
    "\n",
    "<div style=\"width:400 px; font-size:100%; text-align:center;\"> <center><img src=\"img/fig4-19.png\" width=400px alt=\"fig4-19\" style=\"padding-bottom:1.0em;padding-top:2.0em;\"></center>_Figure 4-19. Lasso versus Ridge regularization: white circles show the Batch Gradient Descent path with that cost function. The foreground contours (diamonds/circle) represent the $l_1$/$l_2$ penalty, and the triangles show the BGD path for this penalty only ($\\alpha \\rightarrow \\infty$), left: $\\alpha=0$, right: $\\alpha=0.5$._</div>\n",
    "\n",
    "Top-right, $l_1$ penalty with $\\alpha = 0.5$, The global minimum is on $\\theta_2 = 0$ axis. BGD first reaches $\\theta_2 = 0$, then rolls down the gutter until it reaches the global minimum.\n",
    "\n",
    "On the Lasso cost function, the BGD path tends to bounce across the gutter toward the end. This is because the slope changes abruptly at $\\theta_2 = 0$. You need to gradually reduce the learning rate in order to actually converge to the global minimum.\n",
    "\n",
    "#### Elastic Net\n",
    "\n",
    "$$J(\\theta) = MSE(\\theta) + r\\alpha \\sum^n_{i=1} | \\theta_i | + \\frac{1-r}{2} \\alpha \\sum^n_{i=1} \\theta^2_i$$\n",
    "\n",
    "Ridge is a good default, but if you suspect that only a few features are actually useful, you should prefer Lasso or Elastic Net since they tend to reduce the useless features' weights down to zero. In general, Elastic Net is preferred over Lasso since Lasso may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correlated.\n",
    "\n",
    "#### Early Stopping\n",
    "\n",
    "Stop training as soon as the validation error reaches a minimum. (\"beautiful free lunch\", Geoffrey Hinton)\n",
    "\n",
    "<div style=\"width:400 px; font-size:100%; text-align:center;\"> <center><img src=\"img/fig4-20.png\" width=400px alt=\"fig4-20\" style=\"padding-bottom:1.0em;padding-top:2.0em;\"></center>_Figure 4-20. Early stopping regularization_</div>\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "It is used to estimate the probability that an instance belongs to a particular class.\n",
    "\n",
    "Logistic Regression model estimated probability (vectorized form):\n",
    "\n",
    "$$\\hat{p} = h_{\\theta}(\\mathbf{x}) = \\sigma(\\theta^T \\cdot \\mathbf{x})$$\n",
    "\n",
    "The logistic — also called the _logit_, noted $\\sigma()$ — is a _sigmoid function_ (i.e., S-shaped) that outputs a number between 0 and 1.\n",
    "\n",
    "$$\\sigma(t) = \\frac{1}{1+exp(-t)}$$\n",
    "\n",
    "<div style=\"width:400 px; font-size:100%; text-align:center;\"> <center><img src=\"img/fig4-21.png\" width=400px alt=\"fig4-21\" style=\"padding-bottom:1.0em;padding-top:2.0em;\"></center>_Figure 4-21. Logistic function_</div>\n",
    "\n",
    "$$\\hat{y} = \n",
    "\\left\\{\\begin{matrix}\n",
    "0 \\ if \\hat{p} < 0.5\n",
    "\\\\ \n",
    "1 \\ if \\hat{p} \\geq 0.5\n",
    "\\end{matrix}\\right.$$\n",
    "\n",
    "#### Training and Cost function\n",
    "\n",
    "Logistic Regression cost function (log loss)\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum^m_{i=1}[y^{(i)}log(\\hat{p}^{(i)}) + (1-y^{(i)})log(1-\\hat{p}^{(i)})]$$\n",
    "\n",
    "Bad news: no known closed-form (Normal) equation. Good news: convex, Gradient Descent works.\n",
    "\n",
    "Logistic cost function partial derivatives\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\frac{1}{m} \\sum^m_{i=1} [\\sigma(\\theta^T \\cdot \\mathbf{x}^{(i)}) - y^{(i)}]x^{(i)}_j$$\n",
    "\n",
    "#### Decision Boundaries\n",
    "\n",
    "The dashed line represents the points where the model estimates a $50%$ probability: this is the model’s decision boundary.\n",
    "\n",
    "<div style=\"width:400 px; font-size:100%; text-align:center;\"> <center><img src=\"img/fig4-24.png\" width=400px alt=\"fig4-24\" style=\"padding-bottom:1.0em;padding-top:2.0em;\"></center>_Figure 4-24. Linear decision boundary_</div>\n",
    "\n",
    "#### Softmax Regression\n",
    "\n",
    "Idea: when given an instance $\\mathbf{x}$, the Softmax Regression model first computes a score $s_k (\\mathbf{x})$ for each class $k$, then estimates the probability of each class by applying the _softmax function_ (_normalized exponential_) to the scores.\n",
    "\n",
    "Softmax score for class k\n",
    "\n",
    "$$s_k(\\mathbf{x}) = \\theta_k^T \\cdot \\mathbf{x}$$\n",
    "\n",
    "Softmax function\n",
    "\n",
    "$$\\hat{p}_k = \\sigma(s(\\mathbf{x}))_k = \\frac{exp(s_k(\\mathbf{x}))}{\\sum^K_{j=1} exp(s_j(\\mathbf{x}))}$$\n",
    "\n",
    "Softmax Regression classifier prediction\n",
    "\n",
    "$$\\hat{y} = \\underset{k}{argmax} \\ \\sigma(s(\\mathbf{x}))_k = \\underset{k}{argmax} \\  s_k(\\mathbf{x}) = \\underset{k}{argmax} \\  (\\theta^T_k \\cdot \\mathbf{x})$$\n",
    "\n",
    "Cross entropy cost function:\n",
    "\n",
    "$$J(\\theta) = - \\frac{1}{m} \\sum^m_{i=1} \\sum^K_{k=1} y^{(i)}_k log (\\hat{p}^{(i)}_k) $$\n",
    "\n",
    "Cross entropy gradient vector for class k\n",
    "\n",
    "$$ \\triangledown _{\\theta_k} J(\\theta) = \\frac{1}{m} \\sum^m_{i=1} (\\hat{p}^{(i)}_k - y^{(i)}_k) \\mathbf{x}^{(i)}$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

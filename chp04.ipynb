{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4. Training Models\n",
    "\n",
    "Two different ways to train a Linear Regression model:\n",
    " - Use a direct \"closed-form\" equation that directly computes the model parameters that best fit the model to the training set.\n",
    " - Use an iterative optimization approach, Gradient Descent (GD), Batch GD, Mini-batch GD, and Stochastic GD.\n",
    "\n",
    "Polynomial Regression, more prone to overfitting. Detect overfitting using learning curves, reduce using regularization.\n",
    "\n",
    "Logistic Regression\n",
    "\n",
    "Softmax Regression\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "A linear model makes a prediction by simply computing a weighted sum of the input features plus a constant called the bias term (intercept term).\n",
    "\n",
    "$$\\mathbf{y} = h_\\theta(\\mathbf{x}) = \\theta^T \\cdot \\mathbf{x}$$\n",
    "\n",
    "Cost function\n",
    "\n",
    "$$MSE(\\mathbf{X}, h_\\theta) = \\frac{1}{m} \\sum^m_{i=1}(\\theta^T \\cdot \\mathbf{x}^{(i)} - y_{(i)})^2$$\n",
    "\n",
    "#### The Normal Equation\n",
    "\n",
    "Closed-form solution\n",
    "$$\\hat{\\theta} = (\\mathbf{X}^T \\cdot \\mathbf{X})^{-1} \\cdot \\mathbf{X}^T \\cdot \\mathbf{y}$$\n",
    "\n",
    "#### Computational Complexity\n",
    "\n",
    "<font color=red>_WARNING_</font>\n",
    "\n",
    "The Normal Equation gets very slow when the number of features grows large (e.g., 100,000).\n",
    "\n",
    "On the positive side, this equation is linear with regards to the number of instances and features in the training set (it is $O(m)$), so it handles large training sets efficiently, provided they can fit in memory.\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Concretely, you start by filling $\\theta$ with random values (_random initialization_), and then you improve it gradually, taking one baby step at a time, each step attempting to decrease the cost function, until the algorithm _converges_ to a minimum.\n",
    "\n",
    "_Learning rate_ hyperparameter: small, slow convergence; large, algorithm diverge.\n",
    "\n",
    "Two main challenges with Gradient Descent: \n",
    " - Random initialization, it could converge to a local minimum; \n",
    " - It may take a very long time to cross the plateau, and if you stop too early you will never reach the global minimum.\n",
    "\n",
    "<font color=red>_WARNING_</font>\n",
    "\n",
    "When using Gradient Descent, you should ensure that all features have a similar scale, or else it will take much longer to converge.\n",
    "\n",
    "#### Batch Gradient Descent\n",
    "\n",
    "It involves calculations over the full training set X, at each Gradient Descent step.\n",
    "\n",
    "How to set the number of iterations: set a very large number of iterations but to interrupt the algorithm when the norm of the gradient vector becomes smaller than $\\epsilon$ (tolerance).\n",
    "\n",
    "#### Stochastic Gradient Descent\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

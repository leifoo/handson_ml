{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 8. Dimensionality Reduction\n",
    "\n",
    "Purpose of dimension reduction: \n",
    " - _Curse of dimensionality_: large number of features $\\rightarrow$ training slow, difficult to find a good solution.\n",
    " - Data visualization.\n",
    "\n",
    "Main approaches:\n",
    " - projection\n",
    " - Manifold Learning\n",
    " \n",
    "Techniques:\n",
    " - PCA\n",
    " - Kernel PCA\n",
    " - LLE\n",
    " \n",
    "### The Curse of Dimensionality\n",
    "\n",
    "High-dimensional datasets are at risk of being very sparse. Of course, this also means that a new instance will likely be far away from any training instance, making predictions much less reliable than in lower dimensions, since they will be based on much larger extrapolations. In short, the more dimensions the training set has, the greater the risk of overfitting it.\n",
    "\n",
    "In theory, one solution could be to increase the size of the training set. Unfortunately, in practice, the number of training instances required to reach a given density grows exponentially with the number of dimensions.\n",
    "\n",
    "### Main Approaches for Dimensionality Reduction\n",
    "\n",
    "#### Projection\n",
    "\n",
    "In most real-world problems, training instances are not spread out uniformly across all dimensions. Many features are almost constant, while others are highly correlated. As a result, all training instances actually lie within (or close to) a much lower-dimensional subspace of the high-dimensional space.\n",
    "\n",
    "#### Manifold Learning\n",
    "\n",
    "A $d$-dimensional manifold is a part of an $n$-dimensional space (where $d < n$) that locally resembles a $d$-dimensional hyperplane. (Swiss roll, $d=2, \\ n=3$) \n",
    "\n",
    "_Manifold Learning_: modeling the manifold on which the training instances lie.\n",
    "\n",
    "_Manifold assumption_ (_manifold hypothesis_): most real-world high-dimensional datasets lie close to a much lower-dimensional manifold. (often empirically observed)\n",
    "\n",
    "Another implicit assumption: the task will be simpler if expressed in the lower-dimensional space of the manifold. \n",
    "\n",
    "Reducing the dimensionality of your training set before training a model, will definitely speed up training, but it may not always lead to a better or simpler solution; it all depends on the dataset.\n",
    "\n",
    "### PCA\n",
    "\n",
    "_Principle Component Analysis_ is the most popular dimensionality reduction algorithm. First, identifies the hyperplane that lies closest to the data; Then, projects the data onto it.\n",
    "\n",
    "#### Preserving the Variance\n",
    "\n",
    "Select the axis that\n",
    " - preserves the maximum amount of variance.\n",
    " - minimizes the mean squared distance between the original dataset and its projection onto that axis.\n",
    "\n",
    "#### Principle Components\n",
    "\n",
    "PCA identifies the axis that accounts for the largest amount of variance in the training set, also finds a second axis, orthogonal to the first one, that accounts for the largest amount of remaining variance.\n",
    "\n",
    "_$i^{th}$ principal component_ (PC): unit vector of the $i^{th}$ axis.\n",
    "\n",
    "_Singular Value Decomposition_ (SVD)\n",
    "\n",
    "$$ \\mathbf{X} = \\mathbf{U} \\cdot \\mathbf{\\Sigma} \\cdot \\mathbf{V}^T$$\n",
    "\n",
    "_Principle component matrix_ ($n \\times n$)\n",
    "\n",
    "$$ \\mathbf{V}^T = \n",
    "\\begin{pmatrix}\n",
    " \\vdots & \\vdots  &  & \\vdots \\\\ \n",
    " c_1 & c_2 & \\cdots & c_n \\\\ \n",
    " \\vdots & \\vdots &  & \\vdots\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "<font color=red>_WARNING_</font>\n",
    ">PCA assumes that the dataset is centered around the origin.\n",
    "\n",
    "#### Projecting Down to d Dimensions\n",
    "\n",
    "_Projecting the training set down to $d$ dimensions_\n",
    "\n",
    "$$\\mathbf{X}_{d-proj} = \\mathbf{X} \\cdot \\mathbf{W}_d$$\n",
    "\n",
    "where $\\mathbf{W}_d$ contains the first $d$ principal components\n",
    "\n",
    "#### Explained Variance Ratio\n",
    "\n",
    "The proportion of the dataset's variance that lies along the axis of each principal component.\n",
    "\n",
    "#### Choosing the Right Number of Dimensions\n",
    "\n",
    "Choose the number of dimensions that add up to a sufficiently large portion of the variance (e.g., 95%).\n",
    "\n",
    "Yet another option is to plot the explained variance as a function of the number of dimensions. There will usually be an elbow in the curve, where the explained variance stops growing fast. You can think of this as the intrinsic dimensionality of the dataset.\n",
    "\n",
    "<div style=\"width:400 px; font-size:100%; text-align:center;\"> <center><img src=\"img/fig8-8.png\" width=400px alt=\"fig8-8\" style=\"padding-bottom:1.0em;padding-top:2.0em;\"></center>_Figure 8-8. Explained variance as a function of the number of dimensions_</div>\n",
    "\n",
    "#### PCA for Compression\n",
    "\n",
    "_Reconstruction error_: the mean squared distance between the original data and the reconstructed data (compressed and then decompressed).\n",
    "\n",
    "_PCA inverse transformation, back to the original number of dimensions_\n",
    "\n",
    "$$\\mathbf{X}_{recovered} = \\mathbf{X}_{d-proj} \\cdot \\mathbf{W}^T_d$$\n",
    "\n",
    "#### Incremental PCA\n",
    "\n",
    "Problem: preceding implementation of PCA is that it requires the whole training set to fit in memory in order for the SVD algorithm to run.\n",
    "\n",
    "_Incremental PCA_ (IPCA): split the training set into mini-batches and feed an IPCA algorithm one mini-\n",
    "batch at a time.\n",
    "\n",
    "#### Randomized PCA\n",
    "\n",
    "A stochastic algorithm that quickly finds an approximation of the first $d$ principal components. Computational complexity is $O(m\\times d^2)+O(d^3)$, instead of $O(m\\times n^2)+O(n^3)$, so it is dramatically faster than the previous algorithms when $d$ is much smaller than $n$.\n",
    "\n",
    "### Kernel PCA\n",
    "\n",
    "_Kernel PCA_ (kPCA): the kernel trick can be applied to PCA, making it possible to perform complex nonlinear projections for dimensionality reduction. It is often good at preserving clusters of instances after projection, or sometimes even unrolling datasets that lie close to a twisted manifold.\n",
    "\n",
    "#### Selecting a Kernel and Tuning Hyperparameters\n",
    "\n",
    "kPCA is an unsupervised learning algorithm. Dimensionality reduction is often a preparation step for a supervised learning task (e.g., classification), so you can simply use grid search to select the kernel and hyperparameters.\n",
    "\n",
    "Another approach, this time entirely unsupervised, is to select the kernel and hyperparameters that yield\n",
    "the lowest reconstruction error.\n",
    "\n",
    "<div style=\"width:400 px; font-size:100%; text-align:center;\"> <center><img src=\"img/fig8-11.png\" width=400px alt=\"fig8-11\" style=\"padding-bottom:1.0em;padding-top:2.0em;\"></center>_Figure 8-11. Kernel PCA and the reconstruction pre-image error_</div>\n",
    "\n",
    "### LLE\n",
    "\n",
    "_Locally Linear Embedding_ (LLE) is another very powerful _nonlinear dimensionality reduction_ (NLDR) technique. A Manifold Learning technique that does not rely on projections. \n",
    "\n",
    "First measuring how each training instance linearly relates to its closest neighbors (c.n.), and then looking for a low-dimensional representation of the training set where these local relationships are best preserved. This makes it particularly good at unrolling twisted manifolds, especially when there is not too much noise.\n",
    "\n",
    "The algorithm scale poorly to very large datasets.\n",
    "\n",
    "#### Other Dimensionality Reduction Techniques\n",
    "\n",
    " - Multidimensional Scaling (MDS) reduces dimensionality while trying to preserve the distances between the instances.\n",
    " - Isomap creates a graph by connecting each instance to its nearest neighbors, then reduces dimensionality while trying to preserve the geodesic distances between the instances.\n",
    " - t-Distributed Stochastic Neighbor Embedding (t-SNE) reduces dimensionality while trying to keep similar instances close and dissimilar instances apart. It is mostly used for visualization, in particular to visualize clusters of instances in high-dimensional space (e.g., to visualize the MNIST images in 2D).\n",
    " - Linear Discriminant Analysis (LDA) is actually a classification algorithm, but during training it learns the most discriminative axes between the classes, and these axes can then be used to define a hyperplane onto which to project the data. The benefit is that the projection will keep classes as far apart as possible, so LDA is a good technique to reduce dimensionality before running another classification algorithm such as an SVM classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

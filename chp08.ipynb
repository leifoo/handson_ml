{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 8. Dimensionality Reduction\n",
    "\n",
    "Purpose of dimension reduction: \n",
    " - _Curse of dimensionality_: large number of features $\\rightarrow$ training slow, difficult to find a good solution.\n",
    " - Data visualization.\n",
    "\n",
    "Main approaches:\n",
    " - projection\n",
    " - Manifold Learning\n",
    " \n",
    "Techniques:\n",
    " - PCA\n",
    " - Kernel PCA\n",
    " - LLE\n",
    " \n",
    "### The Curse of Dimensionality\n",
    "\n",
    "High-dimensional datasets are at risk of being very sparse. Of course, this also means that a new instance will likely be far away from any training instance, making predictions much less reliable than in lower dimensions, since they will be based on much larger extrapolations. In short, the more dimensions the training set has, the greater the risk of overfitting it.\n",
    "\n",
    "In theory, one solution could be to increase the size of the training set. Unfortunately, in practice, the number of training instances required to reach a given density grows exponentially with the number of dimensions.\n",
    "\n",
    "### Main Approaches for Dimensionality Reduction\n",
    "\n",
    "#### Projection\n",
    "\n",
    "In most real-world problems, training instances are not spread out uniformly across all dimensions. Many features are almost constant, while others are highly correlated. As a result, all training instances actually lie within (or close to) a much lower-dimensional subspace of the high-dimensional space.\n",
    "\n",
    "#### Manifold Learning\n",
    "\n",
    "A $d$-dimensional manifold is a part of an $n$-dimensional space (where $d < n$) that locally resembles a $d$-dimensional hyperplane. (Swiss roll, $d=2, \\ n=3$) \n",
    "\n",
    "_Manifold Learning_: modeling the manifold on which the training instances lie.\n",
    "\n",
    "_Manifold assumption_ (_manifold hypothesis_): most real-world high-dimensional datasets lie close to a much lower-dimensional manifold. (often empirically observed)\n",
    "\n",
    "Another implicit assumption: the task will be simpler if expressed in the lower-dimensional space of the manifold. \n",
    "\n",
    "Reducing the dimensionality of your training set before training a model, will definitely speed up training, but it may not always lead to a better or simpler solution; it all depends on the dataset.\n",
    "\n",
    "### PCA\n",
    "\n",
    "_Principle Component Analysis_ is the most popular dimensionality reduction algorithm. First, identifies the hyperplane that lies closest to the data; Then, projects the data onto it.\n",
    "\n",
    "#### Preserving the Variance\n",
    "\n",
    "Select the axis that\n",
    " - preserves the maximum amount of variance.\n",
    " - minimizes the mean squared distance between the original dataset and its projection onto that axis.\n",
    "\n",
    "#### Principle Components\n",
    "\n",
    "PCA identifies the axis that accounts for the largest amount of variance in the training set, also finds a second axis, orthogonal to the first one, that accounts for the largest amount of remaining variance.\n",
    "\n",
    "_$i^{th}$ principal component_ (PC): unit vector of the $i^{th}$ axis.\n",
    "\n",
    "_Singular Value Decomposition_ (SVD)\n",
    "\n",
    "$$ \\mathbf{X} = \\mathbf{U} \\cdot \\mathbf{\\Sigma} \\cdot \\mathbf{V}^T$$\n",
    "\n",
    "_Principle component matrix_ ($n \\times n$)\n",
    "\n",
    "$$ \\mathbf{V}^T = \n",
    "\\begin{pmatrix}\n",
    " \\vdots & \\vdots  &  & \\vdots \\\\ \n",
    " c_1 & c_2 & \\cdots & c_n \\\\ \n",
    " \\vdots & \\vdots &  & \\vdots\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "<font color=red>_WARNING_</font>\n",
    ">PCA assumes that the dataset is centered around the origin.\n",
    "\n",
    "#### Projecting Down to d Dimensions\n",
    "\n",
    "_Projecting the training set down to $d$ dimensions_\n",
    "\n",
    "$$\\mathbf{X}_{d-proj} = \\mathbf{X} \\cdot \\mathbf{W}_d$$\n",
    "\n",
    "where $\\mathbf{W}_d$ contains the first $d$ principal components\n",
    "\n",
    "#### Explained Variance Ratio\n",
    "\n",
    "The proportion of the dataset's variance that lies along the axis of each principal component.\n",
    "\n",
    "#### Choosing the Right Number of Dimensions\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

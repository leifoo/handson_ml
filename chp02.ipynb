{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End Machine Learning Project\n",
    "\n",
    "1. Look at the big picture\n",
    "2. Get the data.\n",
    "3. Discover and visualize the data to gain insights.\n",
    "4. Prepare the data for Machine Learning algorithms.\n",
    "5. Select a model and train it.\n",
    "6. Fine-tuning your model.\n",
    "7. Present you solution.\n",
    "8. Launch, monitor, and maintain your system.\n",
    "\n",
    "### Working the Real Data\n",
    "\n",
    "### Look at the Big Picture\n",
    "\n",
    "#### Frame the Problem\n",
    "\n",
    "Questions: 1 objective; 2 current solution; 3 type of problem: supervised, unsupervised, or Reinforcement Learning? classification/regression, batch/online learning? \n",
    "\n",
    "Pipeline: a sequence of data processing components. Each component is self-contained (architecture robust, but uneasy to detect broken component)\n",
    "\n",
    "#### Select a Performance Measure\n",
    "\n",
    "_Root Mean Square Error_ (RMSE) ($l_2$, Euclidian norm) measures _Standard Deviation_ ($\\sigma = \\sqrt{Variance}$):\n",
    "\n",
    "$$RMSE(\\mathbf{X},h) = \\sqrt{\\frac{1}{m} \\sum_{i=1}^m(h(\\mathbf{x}^{(i)}) - y^{(i)})^2}$$\n",
    "\n",
    "$h$ - hypothesis; $\\mathbf{x}$ - matrix with $m$ rows of instances; each column is a feature; $i^{th}$ instance.\n",
    "\n",
    "_Mean Absolute Error_(MAE) (Average Absolute Deviation, $l_1$, Manhattan norm)\n",
    "\n",
    "$$MAE(\\mathbf{X},h) = \\frac{1}{m} \\sum_{i=1}^m | h(\\mathbf{x}^{(i)}) - y^{(i)} | $$\n",
    "\n",
    "$l_k$ norm $||\\mathbf{v}||_k = (|v_0|^k+|v_1|^k + \\cdots + |v_n|^k)^{1/k}$, $l_0$ - cardinality of the vector, $l_\\infty $ - maximum absolute value in the vector. The higher the norm index, the more it focuses on large values and neglects small ones.\n",
    "\n",
    "#### Check the Assumptions\n",
    "\n",
    "### Get the Data\n",
    "\n",
    "#### Create the Workspace\n",
    "\n",
    "#### Download the Data\n",
    "\n",
    "Write scripts to fetch and load data.\n",
    "\n",
    "#### Take a Quick Look at the Data Structure\n",
    "\n",
    "DataFrame's head(), info(), value_counts(), describe() methods.\n",
    "\n",
    "Histogram, hist() method in Matplotlib. Many histograms are _tail heavy_, transform them (e.g., by computing their logarithm).\n",
    "\n",
    "#### Create a Test Set\n",
    "\n",
    "Avoid _data snooping_ bias.\n",
    "\n",
    "When dataset is not large enough (relative to number of attributes), purely random sampling method could introduce a significant sampling bias.\n",
    "\n",
    "_Stratified sampling_: divides the entire population into different homogeneous subgroups called _strata_, then randomly selects the final subjects __proportionally__ from the different strata.\n",
    "\n",
    "### Discover and Visualize the Data to Gain Insights \n",
    "\n",
    "#### Visualizing Geographical Data \n",
    "\n",
    "#### Looking for Correlations \n",
    "\n",
    "_Standard correlation coefficient_ (_Pearson's r_).\n",
    "\n",
    "Note: correlation coefficient only measures linear correlations.\n",
    "\n",
    "corr() method or Pandas' scatter_matrix function\n",
    "\n",
    "#### Experimenting with Attribute Combinations\n",
    "\n",
    "It is an iterative process.\n",
    "\n",
    "### Prepare the Data for Machine Learning Algorithms \n",
    "\n",
    "Write functions to prepare data.\n",
    " - Reproduce these transformations easily on any dataset.\n",
    " - Gradually build a library of transformation functions that you can reuse in future projects. \n",
    " - Use these functions in your live system to transform the new data. \n",
    " - Easily try various transformations and see which combination of transformations works best. \n",
    " \n",
    "#### Data Cleaning\n",
    "\n",
    "If some attribute has some missing values, 3 options to fix this:\n",
    " - Get rid of the corresponding districts. \n",
    " - Get rid of the whole attribute. \n",
    " - Set the values to some value (zero, the mean, the median, etc.).\n",
    "\n",
    "#### Handling Text and Categorical Attributes \n",
    "\n",
    "Convert text labels to numbers.\n",
    "\n",
    "_One-hot encoding_: create one binary attribute per category. \n",
    "\n",
    "#### Custom Transformers\n",
    "\n",
    "Scikit-Learn relies on duck typing (not inheritance), create a class and implement three methods: fit() (returning \n",
    "self), transform(), and fit_transform().\n",
    "\n",
    "#### Feature Scaling\n",
    "\n",
    "Min-max scaling (normalization): \n",
    "\n",
    "$$x' = \\frac{x - x_{min}} {x_{max} - x_{min}} \\in [0,1]$$\n",
    "\n",
    "Standardization:\n",
    "\n",
    "$$x' = \\frac{x - \\bar{x}} {\\sigma}$$\n",
    "\n",
    "Stand deviation: $\\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\bar x)^2}$\n",
    "\n",
    "Unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for \n",
    "some algorithms (e.g., neural networks often expect an input value ranging from 0 to 1). However, standardization is much less affected by outliers. \n",
    "\n",
    "#### Transformation Pipelines \n",
    "\n",
    "### Select and Train a Model \n",
    "\n",
    "#### Training and Evaluating on the Training Set \n",
    "\n",
    "#### Better Evaluation Using Cross-Validation\n",
    "\n",
    "_K-fold cross-validation_: it randomly splits the training set into $K$ distinct subsets called _folds_, then it trains and evaluates the model $K$ times, picking a different fold for evaluation every time and training on the other $K-1$ folds.\n",
    "\n",
    "The goal is to shortlist a few (two to five) promising models, before dive much deeper in particular model.\n",
    "\n",
    "<font color=blue>_TIP_</font>\n",
    ">You should save every model you experiment with, so you can come back easily to any model you want. Make sure you save \n",
    "both the hyperparameters and the trained parameters, as well as the cross-validation scores and perhaps the actual predictions as well. This will allow you to easily compare scores across model types, and compare the types of errors they make. \n",
    "\n",
    "### Fine-Tune Your Model \n",
    "\n",
    "#### Grid Search\n",
    "\n",
    "Bootstrap: random sampling with replacement. (reason: test the stability of a solution)\n",
    "\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

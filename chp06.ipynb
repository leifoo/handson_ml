{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6. Decision Trees\n",
    "\n",
    "Versatile: both classification and regression, even multioutput. \n",
    "\n",
    "### Training and Visualizing a Decision Tree\n",
    "\n",
    "<div style=\"width:400 px; font-size:100%; text-align:center;\"> <center><img src=\"img/fig6-1.png\" width=400px alt=\"fig6-1\" style=\"padding-bottom:1.0em;padding-top:2.0em;\"></center>_Figure 6-1. Iris Decision Tree_</div>\n",
    "\n",
    "Attributes: \n",
    " - _sample_: counts how many training instances it applies to.\n",
    " - _value_: how many training instances of each class this node applies to.\n",
    " - _gini_: measures its impurity\n",
    " \n",
    " Gini impurity\n",
    " \n",
    " $$G_i = 1 - \\sum^n_{k=1} {p_{i,k}}^2 $$\n",
    " \n",
    " $p_{i,k}$ is the ratio of class $k$ instances among the training instances in the $i^{th}$ node. \n",
    "\n",
    "### Making Predictions\n",
    "\n",
    "_Root node_: depth 0, at the top.\n",
    "\n",
    "_Leaf node_: it doesn't have any children nodes. \n",
    "\n",
    "<font color=blue>_NOTE_</font>\n",
    ">One of the many qualities of Decision Trees is that they require very little data preparation. In particular, they don't require feature scaling or centering at all.\n",
    "\n",
    "_White box models_ Vs _black box models_ (Decision Trees Vs Random Forests, neural networks)\n",
    "\n",
    "div style=\"width:400 px; font-size:100%; text-align:center;\"> <center><img src=\"img/fig6-2.png\" width=400px alt=\"fig6-2\" style=\"padding-bottom:1.0em;padding-top:2.0em;\"></center>_Figure 6-2. Decision Tree decision boundaries_</div>\n",
    "\n",
    "### Estimating Class Probabilities\n",
    "\n",
    "A Decision Tree can also estimate the probability that an instance belongs to a particular class $k$: first it\n",
    "traverses the tree to find the leaf node for this instance, and then it returns the ratio of training instances of\n",
    "class $k$ in this node.\n",
    "\n",
    "Notice that the estimated probabilities would be identical with different feature values within same decision boundaries.\n",
    "\n",
    "### The CART Training Algorithm\n",
    "\n",
    "Scikit-Learn uses the _Classification and Regression Tree_ (CART) algorithm to train Decision Trees (\"grow\" trees). \n",
    "\n",
    "The idea is really quite simple: the algorithm splits the training set in two subsets using a single feature $k$ and a threshold $t_k$. It searches for the pair $(k, t_k)$ that produces the purest subsets (weighted by their size).\n",
    "\n",
    "_CART cost function for classification_\n",
    "\n",
    "$$ J(k, t_k) = \\frac{m_{left}}{m} G_{left} + \\frac{m_{right}}{m} G_{right} $$\n",
    "\n",
    "where $G_{left/right}$ measures the impurity of the left/right subset; $m_{left/right}$ is the number of instances in the left/right subset.\n",
    "\n",
    "<font color=red>_WARNING_</font>\n",
    ">As you can see, the CART algorithm is a _greedy algorithm_: it greedily searches for an optimum split at the top level, then repeats the process at each level. It does not check whether or not the split will lead to the lowest possible impurity several levels down. A greedy algorithm often produces a reasonably good solution, but it is not guaranteed to be the optimal solution.\n",
    "\n",
    "Unfortunately, finding the optimal tree is known to be an _NP-Complete_ problem: it requires $O(exp(m))$\n",
    "time, making the problem intractable even for fairly small training sets. This is why we must settle for a\n",
    "\"reasonably good\" solution.\n",
    "\n",
    "### Computational Complexity\n",
    "\n",
    "Decision Trees are generally approximately balanced, so traversing the Decision Tree requires going through roughly $O(log_2(m))$ nodes. Since each node only requires checking the value of one feature, the overall\n",
    "prediction complexity is just $O(log_2(m))$, independent of the number of features. So <font color=blue>predictions are very fast</font>, even when dealing with large training sets.\n",
    "\n",
    "However, the training algorithm compares all features (or less if max_features is set) on all samples at each node. This results in a training complexity of $O(n \\times m \\ log_2(m))$. For small training sets (less than a few thousand instances), Scikit-Learn can speed up training by presorting the data (set presort=True), but this slows down training considerably for larger training sets.\n",
    "\n",
    "### Gini Impurity or Entropy?\n",
    "\n",
    "In Machine Learning, _entropy_ is frequently used as an impurity measure: a set's entropy is zero when it contains instances of only one class.\n",
    "\n",
    "_Entropy_\n",
    "\n",
    "$$H_i = - \\sum^n_{}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

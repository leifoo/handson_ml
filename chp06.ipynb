{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6. Decision Trees\n",
    "\n",
    "Versatile: both classification and regression, even multioutput. \n",
    "\n",
    "### Training and Visualizing a Decision Tree\n",
    "\n",
    "<div style=\"width:400 px; font-size:100%; text-align:center;\"> <center><img src=\"img/fig6-1.png\" width=400px alt=\"fig6-1\" style=\"padding-bottom:1.0em;padding-top:2.0em;\"></center>_Figure 6-1. Iris Decision Tree_</div>\n",
    "\n",
    "Attributes: \n",
    " - _sample_: counts how many training instances it applies to.\n",
    " - _value_: how many training instances of each class this node applies to.\n",
    " - _gini_: measures its impurity\n",
    " \n",
    " Gini impurity\n",
    " \n",
    " $$G_i = 1 - \\sum^n_{k=1} {p_{i,k}}^2 $$\n",
    " \n",
    " $p_{i,k}$ is the ratio of class $k$ instances among the training instances in the $i^{th}$ node. \n",
    "\n",
    "### Making Predictions\n",
    "\n",
    "_Root node_: depth 0, at the top.\n",
    "\n",
    "_Leaf node_: it doesn't have any children nodes. \n",
    "\n",
    "<font color=blue>_NOTE_</font>\n",
    ">One of the many qualities of Decision Trees is that they require very little data preparation. In particular, they don't require feature scaling or centering at all.\n",
    "\n",
    "_White box models_ Vs _black box models_ (Decision Trees Vs Random Forests, neural networks)\n",
    "\n",
    "div style=\"width:400 px; font-size:100%; text-align:center;\"> <center><img src=\"img/fig6-2.png\" width=400px alt=\"fig6-2\" style=\"padding-bottom:1.0em;padding-top:2.0em;\"></center>_Figure 6-2. Decision Tree decision boundaries_</div>\n",
    "\n",
    "### Estimating Class Probabilities\n",
    "\n",
    "A Decision Tree can also estimate the probability that an instance belongs to a particular class $k$: first it\n",
    "traverses the tree to find the leaf node for this instance, and then it returns the ratio of training instances of\n",
    "class $k$ in this node.\n",
    "\n",
    "Notice that the estimated probabilities would be identical with different feature values within same decision boundaries.\n",
    "\n",
    "### The CART Training Algorithm\n",
    "\n",
    "Scikit-Learn uses the _Classification and Regression Tree_ (CART) algorithm to train Decision Trees (\"grow\" trees). \n",
    "\n",
    "The idea is really quite simple: the algorithm splits the training set in two subsets using a single feature $k$ and a threshold $t_k$. It searches for the pair $(k, t_k)$ that produces the purest subsets (weighted by their size).\n",
    "\n",
    "_CART cost function for classification_\n",
    "\n",
    "$$ J(k, t_k) = \\frac{m_{left}}{m} G_{left} + \\frac{m_{right}}{m} G_{right} $$\n",
    "\n",
    "where $G_{left/right}$ measures the impurity of the left/right subset; $m_{left/right}$ is the number of instances in the left/right subset.\n",
    "\n",
    "<font color=red>_WARNING_</font>\n",
    ">As you can see, the CART algorithm is a _greedy algorithm_: it greedily searches for an optimum split at the top level, then repeats the process at each level. It does not check whether or not the split will lead to the lowest possible impurity several levels down. A greedy algorithm often produces a reasonably good solution, but it is not guaranteed to be the optimal solution.\n",
    "\n",
    "Unfortunately, finding the optimal tree is known to be an _NP-Complete_ problem: it requires $O(exp(m))$\n",
    "time, making the problem intractable even for fairly small training sets. This is why we must settle for a\n",
    "\"reasonably good\" solution.\n",
    "\n",
    "P is the set of problems that can be solved in polynomial time. NP is the set of problems whose solutions can be verified in polynomial time. An NP-Hard problem is a problem to which any NP problem can be reduced in polynomial time. An NP-Complete problem is both NP and NP-Hard. A major open mathematical question is whether or not P = NP. If P â‰  NP (which seems likely), then no polynomial algorithm will ever be found for any NP-Complete problem (except perhaps on a quantum computer).\n",
    "\n",
    "### Computational Complexity\n",
    "\n",
    "Decision Trees are generally approximately balanced, so traversing the Decision Tree requires going through roughly $O(log_2(m))$ nodes. Since each node only requires checking the value of one feature, the overall\n",
    "prediction complexity is just $O(log_2(m))$, independent of the number of features. So <font color=blue>predictions are very fast</font>, even when dealing with large training sets.\n",
    "\n",
    "However, the training algorithm compares all features (or less if max_features is set) on all samples at each node. This results in a training complexity of $O(n \\times m \\ log_2(m))$. For small training sets (less than a few thousand instances), Scikit-Learn can speed up training by presorting the data (set presort=True), but this slows down training considerably for larger training sets.\n",
    "\n",
    "### Gini Impurity or Entropy?\n",
    "\n",
    "In Machine Learning, _entropy_ is frequently used as an impurity measure: a set's entropy is zero when it contains instances of only one class.\n",
    "\n",
    "_Entropy_\n",
    "\n",
    "$$H_i = - \\sum^n_{\\begin{matrix}k=1\\\\p_{i,k} \\ne 0 \\end{matrix}} p_{i,k} \\ log(p_{i,k})$$\n",
    "\n",
    "Gini impurity is slightly faster to compute, so it is a good default. However, when they differ, Gini impurity tends to isolate the most frequent class in its own branch of the tree, while entropy tends to produce slightly more balanced trees\n",
    "\n",
    "### Regularization Hyperparameters\n",
    "\n",
    "Decision Trees make very few assumptions about the training data. If left unconstrained, the tree structure will adapt itself to the training data, fitting it very closely, and most likely overfitting it. \n",
    "\n",
    "_nonparametric model_: the number of parameters is not determined prior to training, so the model structure is free to stick closely to the data. \n",
    "\n",
    "_parametric model_: has a predetermined number of parameters, so its degree of freedom is limited, reducing the risk of overfitting (but increasing the risk of underfitting).\n",
    "\n",
    "Regularization:\n",
    " - maximum depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_leaf_nodes, max_features\n",
    " \n",
    "<font color=blue>_NOTE_</font>\n",
    ">Other algorithms work by first training the Decision Tree without restrictions, then pruning (deleting) unnecessary nodes. A node whose children are all leaf nodes is considered unnecessary if the purity improvement it provides is not statistically significant.\n",
    "\n",
    "### Regression\n",
    "\n",
    "Notice how the predicted value for each region is always the average target value of the instances in that region. The algorithm splits each region in a way that makes most training instances as close as possible to that predicted value.\n",
    "\n",
    "_CART cost function for regression_\n",
    "\n",
    "$$ J(k, t_k) = \\frac{m_{left}}{m} MSE_{left} + \\frac{m_{right}}{m} MSE_{right} $$\n",
    "\n",
    "where \n",
    "$\\left\\{\\begin{matrix}\n",
    "MSE_{node} = \\sum_{i \\in node} (\\hat{y}_{node}-y^{(i)})^2 \n",
    "\\\\ \n",
    "\\hat{y}_{node} = \\frac{1}{m_{node}}\\sum_{i \\in node}y^{(i)}\n",
    "\\end{matrix}\\right.$\n",
    "\n",
    "### Instability\n",
    "\n",
    "Decision Trees are simple to understand and interpret, easy to use, versatile, and powerful.\n",
    "\n",
    "Limitations:\n",
    " - Orthogonal decision boundaries (all splits are perpendicular to an axis), which makes them sensitive to training set rotation. (solution: PCA)\n",
    " - Very sensitive to small variations in the training data. (solution: random Forests can limit this instability by averaging predictions over many trees) (the training algorithm used by Scikit-Learn is stochastic)\n",
    " \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

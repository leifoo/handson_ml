{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3. Classification\n",
    "\n",
    "### MNIST\n",
    "\n",
    "Shuffle the training set: 1. guarantee that all cross-validation folds will be similar; 2. some learning algorithms are sensitive to the order of the training instances, and they perform poorly if they get many similar instances in a row.\n",
    "\n",
    "### Training a Binary Classifier\n",
    "\n",
    "Stochastic Gradient Descent_ (SGD) classifier: the true gradient of is approximated by a gradient at a single example.\n",
    "\n",
    "Advantage: capable of handling very large datasets efficiently. \n",
    "\n",
    "This is in part because SGD deals with training instances independently, one at a time (suitable for _online learning_)\n",
    "\n",
    "### Performance Measures\n",
    "\n",
    "#### Measuring Accuracy Using Cross-Validation\n",
    "\n",
    "K-fold cross-validation means splitting the training set into K-folds, then making predictions and evaluating them on each fold using a model trained on the remaining folds.\n",
    "\n",
    "#### Confusion Matrix\n",
    "\n",
    "Each row in a confusion matrix represents an _actual class_, while each column represents a _predicted class_.\n",
    "\n",
    "true negatives (TN) false positives (FP)\n",
    "false negatives (FN) true positives (TP)\n",
    "\n",
    "Precision, positive predictive value (PPV)\n",
    "$$precision = \\frac{TP}{TP+FP}$$\n",
    "\n",
    "Recall, true positive rate (TPR):\n",
    "$$recall = \\frac{TP}{TP+FN}$$\n",
    "\n",
    "<div style=\"width:400 px; font-size:100%; text-align:center;\"> <center><img src=\"img/fig3-2.png\" width=400px alt=\"fig3-2\" style=\"padding-bottom:1.0em;padding-top:2.0em;\"></center>_Figure 3-2. An illustrated confusion matrix_</div>\n",
    "\n",
    "_F-1 score_ is the harmonic mean of precision and recall:\n",
    "\n",
    "$$F_1 = 2 \\frac{precision \\cdot recall}{precision+recall}$$\n",
    "\n",
    "F-measure:\n",
    "$$F_\\beta = (1+\\beta^2) \\cdot \\frac{precision \\cdot recall}{(\\beta^2 \\cdot precision)+recall}$$\n",
    "\n",
    "The $F_1$ score favors classifiers that have similar precision and recall.Two other commonly used F measures are the $F_2$ measure, which weighs recall higher than precision (by placing more emphasis on false negatives), and the $F_{0.5}$ measure, which weighs recall lower than precision (by attenuating the influence of false negatives).\n",
    "\n",
    "#### Precision/Recall Tradeoff\n",
    "\n",
    "Classification, for each instance, it computes a score based on a _decision function_, and if that score is greater than a threshold, it assigns the instance to the positive class, or else it assigns it to the negative class.\n",
    "\n",
    "Lowering the threshold increases recall and reduces precision. \n",
    "\n",
    "Method 1 to decide threshold: precision and recall as functions of the threshold value.\n",
    "\n",
    "<div style=\"width:400 px; font-size:100%; text-align:center;\"> <center><img src=\"img/fig3-4.png\" width=400px alt=\"fig3-4\" style=\"padding-bottom:1.0em;padding-top:2.0em;\"></center>_Figure 3-4. Precision and recall versus the decision threshold_</div>\n",
    "\n",
    "Method 2: plot precision directly against recall.\n",
    "\n",
    "<div style=\"width:400 px; font-size:100%; text-align:center;\"> <center><img src=\"img/fig3-5.png\" width=400px alt=\"fig3-5\" style=\"padding-bottom:1.0em;padding-top:2.0em;\"></center>_Figure 3-5. Precision versus recall_</div>\n",
    "\n",
    "#### The ROC Curve\n",
    "\n",
    "The _receiver operating characteristic_ (ROC) curve is another common tool used with binary classifiers. It plots the _true positive rate_ (TPR, sensitivity, recall) against the _false positive rate_ (FPR), _sensitivity (recall)_ versus _1 â€“ specificity_. \n",
    "\n",
    "$FPR = 1 - TNR$: the ratio of negative instances that are correctly classified as negative. TNR: the ratio of negative instances that are correctly classified as negative. The TNR is also called _specificity_.\n",
    "\n",
    "<div style=\"width:400 px; font-size:100%; text-align:center;\"> <center><img src=\"img/fig3-6.png\" width=400px alt=\"fig3-6\" style=\"padding-bottom:1.0em;padding-top:2.0em;\"></center>_Figure 3-6. ROC curve_</div>\n",
    "\n",
    "One way to compare classifiers is to measure the _area under the curve_ (AUC).\n",
    "\n",
    "<font color=blue>_TIP_</font>\n",
    "Prefer the PR curve whenever the positive class is rare or when you care more about the false positives than the false negatives, and the ROC curve otherwise.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

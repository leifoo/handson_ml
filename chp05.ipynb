{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5. Support Vector Machines\n",
    "\n",
    "Powerful, versatile, linear & nonlinear classifications, regression, outlier detection. \n",
    "\n",
    "Particularly well suited for classification of complex but small- or medium-sized datasets.\n",
    "\n",
    "### Linear SVM Classification\n",
    "\n",
    "_Large margin classification_: fitting the widest possible street (represented by the parallel dashed lines) between the classes.\n",
    "\n",
    "<div style=\"width:600 px; font-size:100%; text-align:center;\"> <center><img src=\"img/fig5-1.png\" width=600px alt=\"fig5-1\" style=\"padding-bottom:1.0em;padding-top:2.0em;\"></center>_Figure 5-1. Large margin classification_</div>\n",
    "\n",
    "Notice that adding more training instances “off the street” will not affect the decision boundary at all: it is\n",
    "fully determined (or \"supported\") by the instances located on the edge of the street. These instances are\n",
    "called the _support vectors_.\n",
    "\n",
    "<font color=red>_WARNING_</font>\n",
    ">SVMs are sensitive to the feature scales.\n",
    "\n",
    "#### Soft Margin Classification\n",
    "\n",
    "_Hard margin classification_: if we strictly impose that all instances be off the street and on the right side. \n",
    "\n",
    "Issues: 1) it only works if the data is linearly separable; 2) sensitive to outliers.\n",
    "\n",
    "_Soft margin classification_: a good balance between keeping the street as large as possible and limiting the _margin violations_.\n",
    "\n",
    "C hyperparameter: a smaller C value leads to a wider street but more margin violations, generalize better: fewer prediction errors.\n",
    "\n",
    "<font color=blue>_TIP_</font>\n",
    ">If your SVM model is overfitting, you can try regularizing it by reducing C .\n",
    "\n",
    "<font color=blue>_NOTE_</font>\n",
    ">Unlike Logistic Regression classifiers, SVM classifiers do not output probabilities for each class.\n",
    "\n",
    "### Nonlinear SVM Classification\n",
    "\n",
    "One approach to handling nonlinear datasets is to add more features, such as polynomial features.\n",
    "\n",
    "<div style=\"width:400 px; font-size:100%; text-align:center;\"> <center><img src=\"img/fig5-6.png\" width=400px alt=\"fig5-6\" style=\"padding-bottom:1.0em;padding-top:2.0em;\"></center>_Figure 5-6. Linear SVM classifier using polynomial features_</div>\n",
    "\n",
    "#### Polynomial Kernel\n",
    "\n",
    "_Kernel trick_\n",
    "\n",
    "Get the same result as if you added many polynomial features, even with very high-degree polynomials, without actually having to add them.\n",
    "\n",
    "#### Adding Similarity Features\n",
    "\n",
    "Another technique to tackle nonlinear problems is to add features computed using a _similarity function_ that measures how much each instance resembles a particular _landmark_.\n",
    "\n",
    "Gaussian Radial Basis Function (RBF)\n",
    "\n",
    "$$ \\phi \\gamma(\\mathbf{x}, \\iota) = exp(-\\gamma \\left \\| \\mathbf{x} - \\iota \\right \\|^2)$$\n",
    "\n",
    "\n",
    "<div style=\"width:600 px; font-size:100%; text-align:center;\"> <center><img src=\"img/fig5-8.png\" width=600px alt=\"fig5-8\" style=\"padding-bottom:1.0em;padding-top:2.0em;\"></center>_Figure 5-8. Similarity features using the Gaussian RBF_</div>\n",
    "\n",
    "The simplest approach to select landmarks is to create a landmark at the location of each and every instance in the dataset. ($\\#$ of features: $n \\rightarrow m$)\n",
    "\n",
    "#### Gaussian RBF Kernel \n",
    "\n",
    "Apply kernel trick.\n",
    "\n",
    "Hyperparameters gamma ($\\gamma$) and C. Increasing gamma makes the bell-shape curve narrower, and as a result each instance's range of influence is smaller: the decision boundary ends up being more irregular, wiggling around individual instances.\n",
    "\n",
    "<font color=blue>_TIP_</font>\n",
    ">Which kernel to use? As a rule of thumb, try linear kernel first, especially if the training set is very large or\n",
    "if it has plenty of features. If the training set is not too large, you should try the Gaussian RBF kernel as well; it works well in most cases.\n",
    "\n",
    "#### Computational Complexity\n",
    "\n",
    "### SVM regression\n",
    "\n",
    "SVM Regression tries to fit as many instances as possible on the street while limiting margin violations. (i.e., instances _off_ the street). The width of the street is controlled by a hyperparameter $\\epsilon$.\n",
    "\n",
    "$\\epsilon$-insensitive model - adding more training instances within the margin does not affect the model's predictions; \n",
    "\n",
    "<font color=blue>_NOTE_</font>\n",
    ">SVMs can also be used for outlier detection.\n",
    "\n",
    "### Under the Hood\n",
    "\n",
    "#### Decision Function and Predictions\n",
    "\n",
    "Linear SVM classifier prediction\n",
    "$$\\hat{y} = \n",
    "\\left\\{\\begin{matrix}\n",
    "0 \\ \\ if \\mathbf{w}^T \\cdot \\mathbf{x} + b < 0\n",
    "\\\\ \n",
    "1 \\ \\ if \\mathbf{w}^T \\cdot \\mathbf{x} + b \\ge 0\n",
    "\\end{matrix}\\right.$$\n",
    "\n",
    "<div style=\"width:400 px; font-size:100%; text-align:center;\"> <center><img src=\"img/fig5-12.png\" width=400px alt=\"fig5-12\" style=\"padding-bottom:1.0em;padding-top:2.0em;\"></center>_Figure 5-12. Decision function for the iris dataset_</div>\n",
    "\n",
    "#### Training Objective\n",
    "\n",
    "Minimize $||w||$ to get a large margin.\n",
    "\n",
    "Hard margin linear SVM classifier objective\n",
    "\n",
    ">$\\underset{\\mathbf{w},b}{minimize} \\frac{1}{2} \\mathbf{w}^T \\cdot \\mathbf{w}$\n",
    "<br>\n",
    ">subject to $\\  t^{(i)}(\\mathbf{w}^T \\cdot \\mathbf{x}^{(i)} + b) \\ge 1$ for $i=1,2,\\cdots , m$\n",
    "\n",
    "$t^{(i)} = -1$ for negative instances ($y^{(i)} = 0$) and $t^{(i)}=1$ for positive instances ($y^{(i)}=1$).\n",
    "\n",
    "For soft margin, _slack variable_ $\\zeta^{(i)} \\ge 0$ for each instance: $\\zeta^{(i)}$ measures how much the i th instance is allowed to violate the margin.\n",
    "\n",
    "Soft margin linear SVM classifier objective\n",
    "\n",
    ">$\\underset{\\mathbf{w},b,\\zeta}{minimize} \\frac{1}{2} \\mathbf{w}^T \\cdot \\mathbf{w} + C \\sum^m_{i=1} \\zeta^{(i)}$\n",
    "<br>\n",
    ">subject to $\\  t^{(i)}(\\mathbf{w}^T \\cdot \\mathbf{x}^{(i)} + b) \\ge 1 - \\zeta^{(i)}$  and  $\\zeta{(i)} \\ge 0$ for $i=1,2,\\cdots , m$\n",
    "\n",
    "#### Quadratic Programming\n",
    "\n",
    "The hard margin and soft margin problems are both convex quadratic optimization problems with linear constraints. Such problems are known as _Quadratic Programming_ (QP) problems.\n",
    "\n",
    "#### The Dual Problem\n",
    "\n",
    "Given a constrained optimization problem, known as the _primal problem_, it is possible to express a different but closely related problem, called its _dual problem_.\n",
    "\n",
    "The dual problem is faster to solve than the primal when the number of training instances is smaller than the number of features. More importantly, it makes the kernel trick possible, while the primal does not.\n",
    "\n",
    "#### Kernelized SVM\n",
    "\n",
    "Kernel trick for a 2nd-degree polynomial mapping\n",
    "\n",
    "$$\\phi (\\mathbf{a})^T \\cdot \\phi(\\mathbf{b}) = (\\mathbf{a}^T \\cdot \\mathbf{b})^2$$\n",
    "\n",
    "$\\phi$ is the 2nd-degree polynomial mapping function.\n",
    "\n",
    "2nd-degree _polynomial kernel_\n",
    "\n",
    "$$K(\\mathbf{a}, \\mathbf{b}) = (\\mathbf{a}^T \\cdot \\mathbf{b})^2$$\n",
    "\n",
    "In Machine Learning, a _kernel_ is a function capable of computing the dot product $\\phi (\\mathbf{a})^T \\cdot \\phi(\\mathbf{b})$ based only on the original vectors $\\mathbf{a}$ and $\\mathbf{b}$, without having to compute the transformation $\\phi$.\n",
    "\n",
    "Common Kernels:\n",
    "\n",
    "Linear: \n",
    ">$K(\\mathbf{a}, \\mathbf{b}) = \\mathbf{a}^T \\cdot \\mathbf{b}$\n",
    "\n",
    "Polynomial: \n",
    ">$K(\\mathbf{a}, \\mathbf{b}) = (\\gamma \\mathbf{a}^T \\cdot \\mathbf{b} + r)^d$\n",
    "\n",
    "Gaussian RBF: \n",
    ">$K(\\mathbf{a}, \\mathbf{b}) = exp(-\\gamma ||\\mathbf{a} - \\mathbf{b}||^2)$\n",
    "\n",
    "Sigmoid:\n",
    ">$K(\\mathbf{a}, \\mathbf{b}) = tanh(\\gamma \\mathbf{a}^T \\cdot \\mathbf{b} + r)$\n",
    "\n",
    "#### Online SVMs\n",
    "\n",
    "For linear SVM classifiers, one method is to use Gradient Descent to minimize the cost function\n",
    "\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2} \\mathbf{w}^T \\cdot \\mathbf{w} + C \\sum^m_{i=1} max(0, 1 - t^{(i)}(\\mathbf{w}^T \\cdot \\mathbf{x}^{(i)} + b))$$\n",
    "\n",
    "The second sum computes the total of all margin violations.\n",
    "\n",
    "_Hinge loss function_\n",
    "\n",
    "<div style=\"width:400 px; font-size:100%; text-align:center;\"> <center><img src=\"img/fig5-hingeloss.png\" width=400px alt=\"fig5-hingeloss\" style=\"padding-bottom:1.0em;padding-top:2.0em;\"></center>_Figure. Hinge loss function_</div>\n",
    "\n",
    "For large-scale nonlinear problems, consider using neural networks instead.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
